{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- GROUPING ------------------------------------------------------------\n",
    "# Grouping according to jet number\n",
    "\n",
    "def grouping(tX):\n",
    "    jet_num_idx = []\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 0))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 1))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 2))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 3))\n",
    "\n",
    "    return jet_num_idx\n",
    "jet_num_idx = grouping(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  4,  5,  6, 12, 23, 24, 25, 26, 27, 28]),\n",
       " array([ 0,  4,  5,  6, 12, 26, 27, 28]),\n",
       " array([0]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.where(tX[jet_num_idx[0], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[1], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[2], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[3], :] == -999)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- IMPUTATION ACCORDING TO JET NUMBER ----------------------------------------------\n",
    "def imputation (data, jet_num_idx):\n",
    "    # Imputation the mass column with the most frequent value\n",
    "    tx_imp = data.copy()\n",
    "    good_idx = np.where(data[:, 0] != -999)\n",
    "    round_values = np.round(data[good_idx, 0]).astype(int)\n",
    "    counts = np.bincount(round_values[0,:])\n",
    "    tx_imp[:, 0] = np.where(tx_imp[:, 0] == -999, np.argmax(counts), tx_imp[:, 0]) \n",
    "\n",
    "    # Imputation of data for jet_num = 0 and jet_num = 1\n",
    "    tx_imp[jet_num_idx[0], :] = np.where(tx_imp[jet_num_idx[0], :] == -999, 0, tx_imp[jet_num_idx[0], :])\n",
    "    tx_imp[jet_num_idx[1], :] = np.where(tx_imp[jet_num_idx[1], :] == -999, 0, tx_imp[jet_num_idx[1], :])\n",
    "\n",
    "    return tx_imp\n",
    "tx = imputation (tX, jet_num_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------- VARIANCE THRESHOLD -----------------------------------------------------------\n",
    "thresh = 0\n",
    "data = tx[jet_num_idx[0][0], :]\n",
    "def variance(data, thresh):\n",
    "    v_vector = np.var(data, axis=0)\n",
    "    index_keep = np.where(v_vector > thresh)\n",
    "    new_data = data[:, index_keep[0]]\n",
    "    \n",
    "    return new_data\n",
    "new_data = variance(data, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------- CORRELATION COEFFICIENT -------------------------------------------------------\n",
    "thresh_corr = 0.8\n",
    "data = tx[jet_num_idx[0][0], :]\n",
    "\n",
    "def correlation(data, thresh_corr):\n",
    "    corr_mat = np.empty([data.shape[1], data.shape[1]])\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(i):\n",
    "            if i != j:\n",
    "                corr_mat[i, j] = np.corrcoef(data[:, i], data[:, j])[0, 1]\n",
    "\n",
    "    index_out1 = np.unique(np.where(corr_mat > 0.8)[0])\n",
    "    index_out2 = np.unique(np.where(corr_mat > 0.8)[1])\n",
    "    all_idx = range(data.shape[1])\n",
    "\n",
    "    if len(index_out1) > len(index_out2):\n",
    "        new_data = data[:, np.setdiff1d(all_idx, index_out1)]\n",
    "    else:\n",
    "        new_data = data[:, np.setdiff1d(all_idx, index_out2)]\n",
    "    \n",
    "    return new_data\n",
    "new_data2 = correlation(new_data, thresh_corr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- STANDARDIZE -------------------------------------------------------------\n",
    "from data_processing import *\n",
    "y_data = y[jet_num_idx[0][0]]\n",
    "tx_std = standardize(new_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of points containing at least one outlier is 7.066%\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------- HANDLING OUTLIERS ----------------------------------------------------------\n",
    "from data_processing import *\n",
    "def remove_outliers(data, y, std_limit = 4):\n",
    "\n",
    "    num_datapoints = np.shape(data)[0]\n",
    "    num_feat = np.shape(data)[1]\n",
    "    indices = np.indices((1,num_datapoints))\n",
    "\n",
    "    standardized = standardize(tx)\n",
    "    number_outliers = np.zeros((1,num_feat))\n",
    "    index_outliers = []\n",
    "\n",
    "    for ii in range(num_feat):    \n",
    "        pos_outlier = standardized[:,ii]>std_limit\n",
    "        neg_outlier = standardized[:,ii]<-std_limit\n",
    "        number_outliers[0,ii] = np.sum(pos_outlier) + np.sum(neg_outlier)\n",
    "    \n",
    "        for jj in range(num_datapoints):\n",
    "            if (pos_outlier[jj] == True or neg_outlier[jj] == True) and jj not in index_outliers:\n",
    "                index_outliers.append(jj)\n",
    "\n",
    "    print(\"Percentage of points containing at least one outlier is\", f'{(100*len(index_outliers)/num_datapoints):.3f}%')\n",
    "    standardized_outliers_removed = standardized[np.setdiff1d(indices,index_outliers)]\n",
    "    y_std = y[np.setdiff1d(indices,index_outliers)]\n",
    "    \n",
    "    return standardized_outliers_removed, y_std\n",
    "tx_std, y_std = remove_outliers(tx, y, std_limit = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: loss=42.5924752980188, w = [-0.48971605 -2.60767668 -0.71779636 -3.21291976 -1.52302557 -0.43929834\n",
      " -0.49430603 -0.34019738 -1.47949577 -0.04899586  0.02464828 -1.9731975\n",
      " -0.03676249  0.00991877  0.02442958 -0.01480398 -1.27873522]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using gradient descent\n",
    "from implementations import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7 # use linspace to test various gamma values and look for the best ?\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx_std.shape[1]+1)\n",
    "tx_offset = np.empty([tx_std.shape[0], tx_std.shape[1]+1])\n",
    "tx_offset[:, 0] = np.ones([tx_std.shape[0]]) \n",
    "tx_offset[:, 1:] = tx_std\n",
    "loss_GD, w_GD = least_squares_GD(y_data, tx_offset, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Gradient Descent: loss={l}, w = {w}\".format(\n",
    "    l=loss_GD, w=w_GD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Visualization\n",
    "# from ipywidgets import IntSlider, interact\n",
    "\n",
    "# def plot_figure(n_iter):\n",
    "#     fig = gradient_descent_visualization(\n",
    "#         gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "#     fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "# interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent: loss=7.2799027472704135e+31, w = [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using stochastic gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7 # use linspace to test various gamma values and look for the best ?\n",
    "\n",
    "# Initialization\n",
    "# initial_w = np.zeros(tx.shape[1]+1)\n",
    "# tx_offset = np.empty([tx.shape[0], tx.shape[1]+1])\n",
    "# tx_offset[:, 0] = np.ones([tx.shape[0]]) \n",
    "# tx_offset[:, 1:] = tx\n",
    "loss_GD, w_GD = least_squares_SGD(y_data, tx_offset, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Stochastic Gradient Descent: loss={l}, w = {w}\".format(\n",
    "    l=loss_GD, w=w_GD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def imputation(tX):\n",
    "#     # Imputation Using Zero\n",
    "#     tx_zeros = np.where(tX == -999, 0, tX) \n",
    "\n",
    "#     # Imputation using the most frequent value (constant)\n",
    "#     col_index = np.unique(np.where(tX == -999)[1])\n",
    "#     tx_mostFrq = tX.copy()\n",
    "#     for index in col_index:\n",
    "#         good_idx = np.where(tX[:, index] != -999)\n",
    "#         round_values = np.round(tX[good_idx, index]).astype(int)\n",
    "    \n",
    "#         # Taking care of the negative values\n",
    "#         neg = round_values[np.where(round_values < 0)]  \n",
    "#         if neg.size > 0:    \n",
    "#             # Positive values\n",
    "#             pos = round_values[np.where(round_values >= 0)]        \n",
    "#             counts_neg = np.bincount(np.abs(neg))\n",
    "#             counts_pos = np.bincount(pos)\n",
    "        \n",
    "#             if max(counts_neg) > max(counts_pos):\n",
    "#                 tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, -np.argmax(counts_neg), tx_mostFrq[:, index])\n",
    "        \n",
    "#             else:\n",
    "#                 tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, np.argmax(counts_pos), tx_mostFrq[:, index])\n",
    "#         else:\n",
    "#             counts = np.bincount(round_values[0,:])\n",
    "#             tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, np.argmax(counts), tx_mostFrq[:, index]) \n",
    "            \n",
    "#     tx_mixed = tX.copy()\n",
    "#     tx_mixed[:, col_index[0]] = tx_mostFrq[:, col_index[0]]\n",
    "#     tx_mixed[:, col_index[1:]] = tx_zeros[:, col_index[1:]]\n",
    "    \n",
    "#     return tx_zeros, tx_mostFrq, tx_mixed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
