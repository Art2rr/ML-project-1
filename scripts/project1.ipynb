{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- GROUPING ------------------------------------------------------------\n",
    "# Grouping according to jet number\n",
    "\n",
    "def grouping(tX):\n",
    "    jet_num_idx = []\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 0))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 1))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 2))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 3))\n",
    "\n",
    "    return jet_num_idx\n",
    "jet_num_idx = grouping(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  4,  5,  6, 12, 23, 24, 25, 26, 27, 28]),\n",
       " array([ 0,  4,  5,  6, 12, 26, 27, 28]),\n",
       " array([0]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.where(tX[jet_num_idx[0], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[1], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[2], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[3], :] == -999)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- IMPUTATION ACCORDING TO JET NUMBER ----------------------------------------------\n",
    "def imputation (data, jet_num_idx):\n",
    "    # Imputation the mass column with the most frequent value\n",
    "    tx_imp = data.copy()\n",
    "    good_idx = np.where(data[:, 0] != -999)\n",
    "    round_values = np.round(data[good_idx, 0]).astype(int)\n",
    "    counts = np.bincount(round_values[0,:])\n",
    "    tx_imp[:, 0] = np.where(tx_imp[:, 0] == -999, np.argmax(counts), tx_imp[:, 0]) \n",
    "\n",
    "    # Imputation of data for jet_num = 0 and jet_num = 1\n",
    "    tx_imp[jet_num_idx[0], :] = np.where(tx_imp[jet_num_idx[0], :] == -999, 0, tx_imp[jet_num_idx[0], :])\n",
    "    tx_imp[jet_num_idx[1], :] = np.where(tx_imp[jet_num_idx[1], :] == -999, 0, tx_imp[jet_num_idx[1], :])\n",
    "\n",
    "    return tx_imp\n",
    "tx = imputation (tX, jet_num_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------- VARIANCE THRESHOLD -----------------------------------------------------------\n",
    "thresh = 0\n",
    "data = tx[jet_num_idx[0][0], :]\n",
    "def variance(data, thresh):\n",
    "    v_vector = np.var(data, axis=0)\n",
    "    index_keep = np.where(v_vector > thresh)\n",
    "    new_data = data[:, index_keep[0]]\n",
    "    \n",
    "    return new_data\n",
    "new_data = variance(data, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------- CORRELATION COEFFICIENT -------------------------------------------------------\n",
    "thresh_corr = 0.8\n",
    "data = tx[jet_num_idx[0][0], :]\n",
    "\n",
    "def correlation(data, thresh_corr):\n",
    "    corr_mat = np.empty([data.shape[1], data.shape[1]])\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(i):\n",
    "            if i != j:\n",
    "                corr_mat[i, j] = np.corrcoef(data[:, i], data[:, j])[0, 1]\n",
    "\n",
    "    index_out1 = np.unique(np.where(corr_mat > 0.8)[0])\n",
    "    index_out2 = np.unique(np.where(corr_mat > 0.8)[1])\n",
    "    all_idx = range(data.shape[1])\n",
    "\n",
    "    if len(index_out1) > len(index_out2):\n",
    "        new_data = data[:, np.setdiff1d(all_idx, index_out1)]\n",
    "    else:\n",
    "        new_data = data[:, np.setdiff1d(all_idx, index_out2)]\n",
    "    \n",
    "    return new_data\n",
    "new_data2 = correlation(new_data, thresh_corr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- STANDARDIZE -------------------------------------------------------------\n",
    "from data_processing import *\n",
    "y_data = y[jet_num_idx[0][0]]\n",
    "tx_std = standardize(new_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#-------------------------------------------------- HANDLING OUTLIERS ----------------------------------------------------------\n",
    "from data_processing import *\n",
    "def remove_outliers(data, y, std_limit = 4):\n",
    "\n",
    "    num_datapoints = np.shape(data)[0]\n",
    "    num_feat = np.shape(data)[1]\n",
    "    indices = np.indices((1,num_datapoints))\n",
    "\n",
    "    standardized = standardize(tx)\n",
    "    number_outliers = np.zeros((1,num_feat))\n",
    "    index_outliers = []\n",
    "\n",
    "    for ii in range(num_feat):    \n",
    "        pos_outlier = standardized[:,ii]>std_limit\n",
    "        neg_outlier = standardized[:,ii]<-std_limit\n",
    "        number_outliers[0,ii] = np.sum(pos_outlier) + np.sum(neg_outlier)\n",
    "    \n",
    "        for jj in range(num_datapoints):\n",
    "            if (pos_outlier[jj] == True or neg_outlier[jj] == True) and jj not in index_outliers:\n",
    "                index_outliers.append(jj)\n",
    "\n",
    "    print(\"Percentage of points containing at least one outlier is\", f'{(100*len(index_outliers)/num_datapoints):.3f}%')\n",
    "    standardized_outliers_removed = standardized[np.setdiff1d(indices,index_outliers)]\n",
    "    y_std = y[np.setdiff1d(indices,index_outliers)]\n",
    "    \n",
    "    return standardized_outliers_removed, y_std\n",
    "tx_std, y_std = remove_outliers(tx, y, std_limit = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: loss=42.59247529801873, w = [-0.48971605 -2.60767668 -0.71779636 -3.21291976 -1.52302557 -0.43929834\n",
      " -0.49430603 -0.34019738 -1.47949577 -0.04899586  0.02464828 -1.9731975\n",
      " -0.03676249  0.00991877  0.02442958 -0.01480398 -1.27873522]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using gradient descent\n",
    "from implementations import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7 # use linspace to test various gamma values and look for the best ?\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx_std.shape[1]+1)\n",
    "tx_offset = np.empty([tx_std.shape[0], tx_std.shape[1]+1])\n",
    "tx_offset[:, 0] = np.ones([tx_std.shape[0]]) \n",
    "tx_offset[:, 1:] = tx_std\n",
    "loss_GD, w_GD = least_squares_GD(y_data, tx_offset, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Gradient Descent: loss={l}, w = {w}\".format(\n",
    "    l=loss_GD, w=w_GD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Visualization\n",
    "# from ipywidgets import IntSlider, interact\n",
    "\n",
    "# def plot_figure(n_iter):\n",
    "#     fig = gradient_descent_visualization(\n",
    "#         gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "#     fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "# interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent: loss=1.7966466423537952e+39, w = [ 5.55257852e+18  1.40322112e+19  4.10112091e+18  1.33253907e+19\n",
      "  3.89699408e+18 -3.49941072e+18 -4.31707316e+18 -2.95560269e+18\n",
      "  1.60751829e+19 -1.26417007e+19 -5.22408568e+18  1.93740288e+19\n",
      " -1.08678164e+19  3.37890042e+18 -4.02084985e+18 -6.11775888e+18\n",
      "  1.02589129e+19]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using stochastic gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7 # use linspace to test various gamma values and look for the best ?\n",
    "\n",
    "# Initialization\n",
    "# initial_w = np.zeros(tx.shape[1]+1)\n",
    "# tx_offset = np.empty([tx.shape[0], tx.shape[1]+1])\n",
    "# tx_offset[:, 0] = np.ones([tx.shape[0]]) \n",
    "# tx_offset[:, 1:] = tx\n",
    "loss_GD, w_GD = least_squares_SGD(y_data, tx_offset, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Stochastic Gradient Descent: loss={l}, w = {w}\".format(\n",
    "    l=loss_GD, w=w_GD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    z = np.squeeze(t)\n",
    "    #sigma = np.where(z>=0,1/(1+np.exp(-z)),np.exp(z)/(1+np.exp(z)))\n",
    "    z_bound = np.where(z<-100,-100,1)\n",
    "    sigma = 1.0 / (1+np.exp(-z_bound))\n",
    "    sigma = np.expand_dims(sigma, axis=1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_logistic_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    sigma = sigmoid(tx.dot(w))\n",
    "    loss = (y.T.dot(np.log(sigmoid(tx.dot(w)))) + (1-y).T.dot(np.log(1-sigmoid(tx.dot(w)))))\n",
    "    return np.squeeze(-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_logistic_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    sigma = sigmoid(tx.dot(w))\n",
    "    ty = np.expand_dims(y,axis=1)\n",
    "    grad = tx.T.dot((np.subtract(sigma, ty)))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    penalty = np.squeeze(lambda_*w.T.dot(w))\n",
    "    loss = calculate_logistic_loss(y,tx,w) + penalty \n",
    "    grad = calculate_logistic_gradient(y,tx,w) #+ 2*lambda_*w\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_GD(y, tx, gamma, lambda_, max_iter=10000):\n",
    "    \n",
    "    #initialise w\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    losses = []\n",
    "\n",
    "    #logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        \n",
    "        # get loss and update w.\n",
    "        loss, grad = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "        w = w - (gamma * grad)\n",
    "        \n",
    "        # decrease step size\n",
    "        if iter % 10 == 0:\n",
    "            gamma = gamma*0.5\n",
    "            \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    l = calculate_logistic_loss(y, tx, w)\n",
    "    print(\"Final loss={l}\".format(l=calculate_logistic_loss(y, tx, w)))\n",
    "    return l, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_SGD(y, tx, gamma, lambda_, max_iter=10000, batch_size=1):\n",
    "    \n",
    "    #initialise w\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "    losses = []\n",
    "\n",
    "    #logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        \n",
    "        # get loss and update w.\n",
    "        for y_b, tx_b in batch_iter(y, tx, batch_size):\n",
    "            loss, grad = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "            w = w - (gamma * grad)\n",
    "        \n",
    "        # decrease step size\n",
    "        if iter % 10 == 0:\n",
    "            gamma = gamma*0.5\n",
    "            \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    l = calculate_logistic_loss(y, tx, w)\n",
    "    print(\"Final loss={l}\".format(l=calculate_logistic_loss(y, tx, w)))\n",
    "    return l, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y,k_fold,seed):\n",
    "    \n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    \n",
    "    return np.array(k_indices)\n",
    "\n",
    "def logistic_cross_validation(y, x, k_indices, k, gamma, lambda_):\n",
    "   \n",
    "    train_indices = np.setdiff1d(k_indices,k_indices[k])\n",
    "    \n",
    "    x_test = x[k_indices[k],:]\n",
    "    x_train = x[train_indices,:]\n",
    "    y_test = y[k_indices[k]]\n",
    "    y_train = y[train_indices]\n",
    "\n",
    "    loss_train,w_optimal = logistic_regression_GD(y_train,x_train, gamma, lambda_, max_iter=10000)\n",
    "    pred = predict_labels(w_optimal,x_test)\n",
    "    pred_bin = np.squeeze(1*np.equal(pred,1))\n",
    "    correct = np.sum(1*np.equal(y_test,pred_bin))\n",
    "    all_ = np.shape(y_test)[0]\n",
    "    acc =  correct/all_\n",
    "    print(f\"The accuracy is\",acc) \n",
    "\n",
    "    return loss_train, acc\n",
    "\n",
    "def logistic_lambda_optimisation(y,x,k,gamma,seed=1):\n",
    "    \n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    k_indices = build_k_indices(y,k,seed)\n",
    "    accuracy = []\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        accuracy_intermediate = []\n",
    "    \n",
    "        for ii in range(k):\n",
    "            print(f\"Doing fold\",ii,\"for lambda value\",lambda_) \n",
    "            loss_tr_ii, acc_test = logistic_cross_validation(y, x, k_indices, ii, gamma, lambda_)\n",
    "            accuracy_intermediate.append(acc_test)\n",
    "        \n",
    "        accuracy.append(np.mean(accuracy_intermediate))\n",
    "    \n",
    "    idx_min_acc = np.argmin(accuracy)\n",
    "    lambda_optimal = lambdas[idx_min_acc]\n",
    "    \n",
    "    return lambda_optimal\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.expand_dims(y, axis=1)\n",
    "y_bin = 1*np.equal(y_data,1)\n",
    "np.shape(tx_std)\n",
    "threshold = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25492"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_sqeezed = np.squeeze(y_data)\n",
    "#y_bin_squeezd = np.squeeze(y_bin)\n",
    "np.sum(1*np.equal(y_data,y_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing fold 0 for lambda value 0.0001\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0001\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546908.3358371903\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0001\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0001\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0001\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.00013738237958832623\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.00013738237958832623\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546908.3358371903\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.00013738237958832623\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.00013738237958832623\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.00013738237958832623\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.00018873918221350977\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.00018873918221350977\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546908.3358371903\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.00018873918221350977\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.00018873918221350977\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.00018873918221350977\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0002592943797404667\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0002592943797404667\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546908.3358371903\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0002592943797404667\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0002592943797404667\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0002592943797404667\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0003562247890262444\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0003562247890262444\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546908.3358371903\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0003562247890262444\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0003562247890262444\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0003562247890262444\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0004893900918477494\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0004893900918477494\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546908.3358371903\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0004893900918477494\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0004893900918477494\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0004893900918477494\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0006723357536499335\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0006723357536499335\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=547185.0706034399\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0006723357536499335\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0006723357536499335\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0006723357536499335\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0009236708571873865\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0009236708571873865\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=547185.0706034399\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0009236708571873865\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0009236708571873865\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0009236708571873865\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0012689610031679222\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0012689610031679222\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=547185.0706034399\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0012689610031679222\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0012689610031679222\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0012689610031679222\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0017433288221999873\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0017433288221999873\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546690.5766969401\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0017433288221999873\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0017433288221999873\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0017433288221999873\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.002395026619987486\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543831.4691741293\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.002395026619987486\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546690.5766969401\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.002395026619987486\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542397.8319038821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.002395026619987486\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547697.578137068\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.002395026619987486\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.0032903445623126675\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543714.7100338791\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.0032903445623126675\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546690.5766969401\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.0032903445623126675\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542692.9523337572\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.0032903445623126675\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547401.1444455056\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.0032903445623126675\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.004520353656360241\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543714.7100338791\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.004520353656360241\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546690.5766969401\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.004520353656360241\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542692.9523337572\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.004520353656360241\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547401.1444455056\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.004520353656360241\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546076.1924734428\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.006210169418915616\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543612.3967721916\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.006210169418915616\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546487.2634352526\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.006210169418915616\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542992.0125486946\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.006210169418915616\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547401.1444455056\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.006210169418915616\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546371.3129033176\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.008531678524172805\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543612.3967721916\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.008531678524172805\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546487.2634352526\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.008531678524172805\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542992.0125486946\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.008531678524172805\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547401.1444455056\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.008531678524172805\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546371.3129033176\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.011721022975334805\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.011721022975334805\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546586.9501735651\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.011721022975334805\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542993.3258103821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.011721022975334805\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547500.831183818\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.011721022975334805\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546371.3129033176\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.01610262027560939\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.01610262027560939\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Final loss=546586.9501735651\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.01610262027560939\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Final loss=542993.3258103821\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.01610262027560939\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Final loss=547500.831183818\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.01610262027560939\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Final loss=546371.3129033176\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.02212216291070448\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=543846.2131934641\n",
      "Final loss=543616.3365572542\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.02212216291070448\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=546917.5052612387\n",
      "Final loss=546686.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.02212216291070448\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=543027.6071548971\n",
      "Final loss=542797.8921188195\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.02212216291070448\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=547832.6778547611\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.02212216291070448\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=546598.2754785998\n",
      "Final loss=546368.6863799426\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.03039195382313198\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=543932.146550197\n",
      "Final loss=543616.3365572542\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.03039195382313198\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=547003.3184154772\n",
      "Final loss=546686.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.03039195382313198\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=543113.480101655\n",
      "Final loss=542797.8921188195\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.03039195382313198\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=547918.482905091\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.03039195382313198\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=546684.1013469533\n",
      "Final loss=546368.6863799426\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.041753189365604\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=544050.2038405368\n",
      "Final loss=543616.3365572542\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.041753189365604\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=547121.2105687697\n",
      "Final loss=546686.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.041753189365604\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=543231.4543993338\n",
      "Final loss=542797.8921188195\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.041753189365604\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=548036.3639250412\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.041753189365604\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=546802.0109671997\n",
      "Final loss=546368.6863799426\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.05736152510448681\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=544212.393755283\n",
      "Final loss=543616.3365572542\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.05736152510448681\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=547283.173614311\n",
      "Final loss=546685.32365019\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.05736152510448681\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=543393.5302967874\n",
      "Final loss=542696.8921188195\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.05736152510448681\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=548198.3116753318\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.05736152510448681\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=546963.9980092577\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.07880462815669913\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=544435.2141196136\n",
      "Final loss=543616.3365572542\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.07880462815669913\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=547505.6823003293\n",
      "Final loss=546685.32365019\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.07880462815669913\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=543616.1940214484\n",
      "Final loss=542696.8921188195\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.07880462815669913\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=548420.7993483708\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.07880462815669913\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=547186.5396622618\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.1082636733874054\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=544741.3300383386\n",
      "Final loss=543617.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.1082636733874054\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=547811.370027972\n",
      "Final loss=546785.0103885025\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.1082636733874054\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=543922.0947448675\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.1082636733874054\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=548726.4582078824\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.1082636733874054\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=547492.2726807339\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.14873521072935117\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=545161.8793717814\n",
      "Final loss=543617.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.14873521072935117\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=548231.3311023171\n",
      "Final loss=546785.0103885025\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.14873521072935117\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=544342.3484378788\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.14873521072935117\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=549146.3796225019\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.14873521072935117\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=547912.2959766982\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.20433597178569418\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=545739.640053408\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.20433597178569418\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=548808.2836195971\n",
      "Final loss=546787.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.20433597178569418\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=544919.7029616454\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.20433597178569418\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=549723.2776543073\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.20433597178569418\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=548489.3339755193\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.2807216203941176\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=546533.3814261524\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.2807216203941176\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=549600.914716931\n",
      "Final loss=546787.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.2807216203941176\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=545712.8863450568\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.2807216203941176\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=550515.8338981997\n",
      "Final loss=547503.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.2807216203941176\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=549282.0825094284\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.38566204211634725\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=547623.8422118057\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.38566204211634725\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=550689.8501798054\n",
      "Final loss=546787.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.38566204211634725\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=546802.5805516867\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.38566204211634725\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=551604.6665256349\n",
      "Final loss=547502.1444455057\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.38566204211634725\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=550371.1793094644\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.5298316906283708\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=549121.943187614\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.5298316906283708\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=552185.8556308834\n",
      "Final loss=546787.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.5298316906283708\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=548299.6283829908\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.5298316906283708\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=553100.5306989396\n",
      "Final loss=547502.1444455057\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.5298316906283708\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=551867.4064093742\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 0.7278953843983146\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=551180.069956815\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 0.7278953843983146\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=554241.1035183455\n",
      "Final loss=546787.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 0.7278953843983146\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=550356.3083172119\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 0.7278953843983146\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=555155.5844956348\n",
      "Final loss=547402.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 0.7278953843983146\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=553922.9588032756\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "Doing fold 0 for lambda value 1.0\n",
      "Current iteration=0, loss=84602.38015995664\n",
      "Current iteration=100, loss=554007.5734872879\n",
      "Final loss=543516.6498189417\n",
      "The accuracy is 0.7435191672505255\n",
      "Doing fold 1 for lambda value 1.0\n",
      "Current iteration=0, loss=84670.38015995662\n",
      "Current iteration=100, loss=557064.6519725797\n",
      "Final loss=546787.6369118775\n",
      "The accuracy is 0.7401161044940446\n",
      "Doing fold 2 for lambda value 1.0\n",
      "Current iteration=0, loss=84489.38015995664\n",
      "Current iteration=100, loss=553181.8241513605\n",
      "Final loss=542799.205380507\n",
      "The accuracy is 0.749174256831148\n",
      "Doing fold 3 for lambda value 1.0\n",
      "Current iteration=0, loss=84550.38015995664\n",
      "Current iteration=100, loss=557978.8663033549\n",
      "Final loss=547402.4577071932\n",
      "The accuracy is 0.7461215093584226\n",
      "Doing fold 4 for lambda value 1.0\n",
      "Current iteration=0, loss=84563.38015995664\n",
      "Current iteration=100, loss=556746.9255957023\n",
      "Final loss=546470.9996416301\n",
      "The accuracy is 0.7455209688719848\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "lbd = logistic_lambda_optimisation(y_bin, tx_offset, 5,0.001)\n",
    "print(lbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    batch_size = 1000\n",
    "    gamma = 0.001\n",
    "    lambda_ = 0.5\n",
    "    threshold = 1e-3\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_log_SGD, w_log_GD = logistic_regression_SGD(y_bin, tx_offset, gamma, lambda_, max_iter, batch_size)\n",
    "print(\"Stochastic Gradient Descent: loss={l}, w = {w}\".format(\n",
    "    l=l_log_SGD, w=w_log_GD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_losses = []\n",
    "test_val = [1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2]\n",
    "for i in test_val:\n",
    "    lambda_ = i\n",
    "    model, final_loss = logistic_regression_GD(y_bin, tx_offset, gamma, lambda_, batch_size, max_iter,stoch=False)\n",
    "    fin_losses.append(final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def imputation(tX):\n",
    "#     # Imputation Using Zero\n",
    "#     tx_zeros = np.where(tX == -999, 0, tX) \n",
    "\n",
    "#     # Imputation using the most frequent value (constant)\n",
    "#     col_index = np.unique(np.where(tX == -999)[1])\n",
    "#     tx_mostFrq = tX.copy()\n",
    "#     for index in col_index:\n",
    "#         good_idx = np.where(tX[:, index] != -999)\n",
    "#         round_values = np.round(tX[good_idx, index]).astype(int)\n",
    "    \n",
    "#         # Taking care of the negative values\n",
    "#         neg = round_values[np.where(round_values < 0)]  \n",
    "#         if neg.size > 0:    \n",
    "#             # Positive values\n",
    "#             pos = round_values[np.where(round_values >= 0)]        \n",
    "#             counts_neg = np.bincount(np.abs(neg))\n",
    "#             counts_pos = np.bincount(pos)\n",
    "        \n",
    "#             if max(counts_neg) > max(counts_pos):\n",
    "#                 tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, -np.argmax(counts_neg), tx_mostFrq[:, index])\n",
    "        \n",
    "#             else:\n",
    "#                 tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, np.argmax(counts_pos), tx_mostFrq[:, index])\n",
    "#         else:\n",
    "#             counts = np.bincount(round_values[0,:])\n",
    "#             tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, np.argmax(counts), tx_mostFrq[:, index]) \n",
    "            \n",
    "#     tx_mixed = tX.copy()\n",
    "#     tx_mixed[:, col_index[0]] = tx_mostFrq[:, col_index[0]]\n",
    "#     tx_mixed[:, col_index[1:]] = tx_zeros[:, col_index[1:]]\n",
    "    \n",
    "#     return tx_zeros, tx_mostFrq, tx_mixed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
