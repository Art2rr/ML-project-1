{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = 'train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------- GROUPING ------------------------------------------------------------\n",
    "# Grouping according to jet number\n",
    "\n",
    "def grouping(tX):\n",
    "    jet_num_idx = []\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 0))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 1))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 2))\n",
    "    jet_num_idx.append(np.where(tX[:,22] == 3))\n",
    "\n",
    "    return jet_num_idx\n",
    "jet_num_idx = grouping(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  4,  5,  6, 12, 23, 24, 25, 26, 27, 28]),\n",
       " array([ 0,  4,  5,  6, 12, 26, 27, 28]),\n",
       " array([0]),\n",
       " array([0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.where(tX[jet_num_idx[0], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[1], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[2], :] == -999)[2]), np.unique(np.where(tX[jet_num_idx[3], :] == -999)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- IMPUTATION ACCORDING TO JET NUMBER ----------------------------------------------\n",
    "def imputation (data, jet_num_idx):\n",
    "    # Imputation the mass column with the most frequent value\n",
    "    tx_imp = data.copy()\n",
    "    good_idx = np.where(data[:, 0] != -999)\n",
    "    round_values = np.round(data[good_idx, 0]).astype(int)\n",
    "    counts = np.bincount(round_values[0,:])\n",
    "    tx_imp[:, 0] = np.where(tx_imp[:, 0] == -999, np.argmax(counts), tx_imp[:, 0]) \n",
    "\n",
    "    # Imputation of data for jet_num = 0 and jet_num = 1\n",
    "    tx_imp[jet_num_idx[0], :] = np.where(tx_imp[jet_num_idx[0], :] == -999, 0, tx_imp[jet_num_idx[0], :])\n",
    "    tx_imp[jet_num_idx[1], :] = np.where(tx_imp[jet_num_idx[1], :] == -999, 0, tx_imp[jet_num_idx[1], :])\n",
    "\n",
    "    return tx_imp\n",
    "tx = imputation (tX, jet_num_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------- VARIANCE THRESHOLD -----------------------------------------------------------\n",
    "thresh = 0\n",
    "data = tx[jet_num_idx[0][0], :]\n",
    "def variance(data, thresh):\n",
    "    v_vector = np.var(data, axis=0)\n",
    "    index_keep = np.where(v_vector > thresh)\n",
    "    new_data = data[:, index_keep[0]]\n",
    "    \n",
    "    return new_data\n",
    "new_data = variance(data, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------- CORRELATION COEFFICIENT -------------------------------------------------------\n",
    "thresh_corr = 0.8\n",
    "data = tx[jet_num_idx[0][0], :]\n",
    "\n",
    "def correlation(data, thresh_corr):\n",
    "    corr_mat = np.empty([data.shape[1], data.shape[1]])\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(i):\n",
    "            if i != j:\n",
    "                corr_mat[i, j] = np.corrcoef(data[:, i], data[:, j])[0, 1]\n",
    "\n",
    "    index_out1 = np.unique(np.where(corr_mat > 0.8)[0])\n",
    "    index_out2 = np.unique(np.where(corr_mat > 0.8)[1])\n",
    "    all_idx = range(data.shape[1])\n",
    "\n",
    "    if len(index_out1) > len(index_out2):\n",
    "        new_data = data[:, np.setdiff1d(all_idx, index_out1)]\n",
    "    else:\n",
    "        new_data = data[:, np.setdiff1d(all_idx, index_out2)]\n",
    "    \n",
    "    return new_data\n",
    "new_data2 = correlation(new_data, thresh_corr=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------- STANDARDIZE -------------------------------------------------------------\n",
    "from data_processing import *\n",
    "y_data = y[jet_num_idx[0][0]]\n",
    "tx_std = standardize(new_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of points containing at least one outlier is 7.066%\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------- HANDLING OUTLIERS ----------------------------------------------------------\n",
    "from data_processing import *\n",
    "def remove_outliers(data, y, std_limit = 4):\n",
    "\n",
    "    num_datapoints = np.shape(data)[0]\n",
    "    num_feat = np.shape(data)[1]\n",
    "    indices = np.indices((1,num_datapoints))\n",
    "\n",
    "    standardized = standardize(tx)\n",
    "    number_outliers = np.zeros((1,num_feat))\n",
    "    index_outliers = []\n",
    "\n",
    "    for ii in range(num_feat):    \n",
    "        pos_outlier = standardized[:,ii]>std_limit\n",
    "        neg_outlier = standardized[:,ii]<-std_limit\n",
    "        number_outliers[0,ii] = np.sum(pos_outlier) + np.sum(neg_outlier)\n",
    "    \n",
    "        for jj in range(num_datapoints):\n",
    "            if (pos_outlier[jj] == True or neg_outlier[jj] == True) and jj not in index_outliers:\n",
    "                index_outliers.append(jj)\n",
    "\n",
    "    print(\"Percentage of points containing at least one outlier is\", f'{(100*len(index_outliers)/num_datapoints):.3f}%')\n",
    "    standardized_outliers_removed = standardized[np.setdiff1d(indices,index_outliers)]\n",
    "    y_std = y[np.setdiff1d(indices,index_outliers)]\n",
    "    \n",
    "    return standardized_outliers_removed, y_std\n",
    "tx_std, y_std = remove_outliers(tx, y, std_limit = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: loss=42.5924752980188, w = [-0.48971605 -2.60767668 -0.71779636 -3.21291976 -1.52302557 -0.43929834\n",
      " -0.49430603 -0.34019738 -1.47949577 -0.04899586  0.02464828 -1.9731975\n",
      " -0.03676249  0.00991877  0.02442958 -0.01480398 -1.27873522]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using gradient descent\n",
    "from implementations import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7 # use linspace to test various gamma values and look for the best ?\n",
    "\n",
    "# Initialization\n",
    "initial_w = np.zeros(tx_std.shape[1]+1)\n",
    "tx_offset = np.empty([tx_std.shape[0], tx_std.shape[1]+1])\n",
    "tx_offset[:, 0] = np.ones([tx_std.shape[0]]) \n",
    "tx_offset[:, 1:] = tx_std\n",
    "loss_GD, w_GD = least_squares_GD(y_data, tx_offset, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Gradient Descent: loss={l}, w = {w}\".format(\n",
    "    l=loss_GD, w=w_GD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Visualization\n",
    "# from ipywidgets import IntSlider, interact\n",
    "\n",
    "# def plot_figure(n_iter):\n",
    "#     fig = gradient_descent_visualization(\n",
    "#         gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "#     fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "# interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent: loss=7.2799027472704135e+31, w = [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# Linear regression using stochastic gradient descent\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7 # use linspace to test various gamma values and look for the best ?\n",
    "\n",
    "# Initialization\n",
    "# initial_w = np.zeros(tx.shape[1]+1)\n",
    "# tx_offset = np.empty([tx.shape[0], tx.shape[1]+1])\n",
    "# tx_offset[:, 0] = np.ones([tx.shape[0]]) \n",
    "# tx_offset[:, 1:] = tx\n",
    "loss_GD, w_GD = least_squares_SGD(y_data, tx_offset, initial_w, max_iters, gamma)\n",
    "\n",
    "print(\"Stochastic Gradient Descent: loss={l}, w = {w}\".format(\n",
    "    l=loss_GD, w=w_GD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    z = np.squeeze(t)\n",
    "    z_bound = np.where(z<-100,-100,1)\n",
    "    sigma = 1.0 / (1+np.exp(-z_bound))\n",
    "    sigma = np.expand_dims(sigma, axis=1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_logistic_loss(y, tx, w):\n",
    "    \"\"\"compute the loss: negative log likelihood.\"\"\"\n",
    "    sigma = sigmoid(tx.dot(w))\n",
    "    loss = (y.T.dot(np.log(sigmoid(tx.dot(w)))) + (1-y).T.dot(np.log(1-sigmoid(tx.dot(w)))))\n",
    "    return np.squeeze(-loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_logistic_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    sigma = sigmoid(tx.dot(w))\n",
    "    ty = np.expand_dims(y,axis=1)\n",
    "    grad = tx.T.dot((np.subtract(sigma, ty)))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    penalty = np.squeeze(lambda_*w.T.dot(w))\n",
    "    loss = calculate_logistic_loss(y,tx,w) + penalty \n",
    "    grad = calculate_logistic_gradient(y,tx,w) #+ 2*lambda_*w\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    Example of use :\n",
    "    for minibatch_y, minibatch_tx in batch_iter(y, tx, 32):\n",
    "        <DO-SOMETHING>\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_GD(y, tx, gamma, lambda_, batch_size, max_iter=10000,stoch=True):\n",
    "    \n",
    "    #initialise w\n",
    "    w = np.zeros((tx.shape[1], 1))\n",
    "\n",
    "    #logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        \n",
    "        # get loss and update w.\n",
    "        if stoch == True:\n",
    "            for y_b, tx_b in batch_iter(y, tx, batch_size):\n",
    "                loss, grad = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "                w = w - (gamma * grad)\n",
    "        else:\n",
    "            loss, grad = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "            w = w - (gamma * grad)\n",
    "        \n",
    "        # decrease step size\n",
    "        if iter % 10 == 0:\n",
    "            gamma = gamma*0.5\n",
    "            \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    l = calculate_loss(y, tx, w)\n",
    "    print(\"Final loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return w, l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99913, 16)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.expand_dims(y, axis=1)\n",
    "y_bin = 1*np.equal(y_data,1)\n",
    "np.shape(tx_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    batch_size = 1000\n",
    "    gamma = 0.001\n",
    "    lambda_ = 0.5\n",
    "    threshold = 1e-3\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=105719.91498500809\n",
      "Final loss=682413.7339582554\n",
      "Current iteration=0, loss=105719.91498500809\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_360/1166930527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlambda_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfin_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_360/2612040280.py\u001b[0m in \u001b[0;36mlogistic_regression_GD\u001b[0;34m(y, tx, gamma, lambda_, batch_size, max_iter, stoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx_b\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_360/888155810.py\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpenalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_logistic_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_logistic_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#+ 2*lambda_*w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_360/3525313113.py\u001b[0m in \u001b[0;36mcalculate_logistic_loss\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"compute the loss: negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fin_losses = []\n",
    "test_val = [1e-5,1e-4,1e-3,1e-2,1e-1,1,1e1,1e2]\n",
    "for i in test_val:\n",
    "    lambda_ = i\n",
    "    model, final_loss = logistic_regression_GD(y_bin, tx_offset, gamma, lambda_, batch_size, max_iter,stoch=False)\n",
    "    fin_losses.append(final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def imputation(tX):\n",
    "#     # Imputation Using Zero\n",
    "#     tx_zeros = np.where(tX == -999, 0, tX) \n",
    "\n",
    "#     # Imputation using the most frequent value (constant)\n",
    "#     col_index = np.unique(np.where(tX == -999)[1])\n",
    "#     tx_mostFrq = tX.copy()\n",
    "#     for index in col_index:\n",
    "#         good_idx = np.where(tX[:, index] != -999)\n",
    "#         round_values = np.round(tX[good_idx, index]).astype(int)\n",
    "    \n",
    "#         # Taking care of the negative values\n",
    "#         neg = round_values[np.where(round_values < 0)]  \n",
    "#         if neg.size > 0:    \n",
    "#             # Positive values\n",
    "#             pos = round_values[np.where(round_values >= 0)]        \n",
    "#             counts_neg = np.bincount(np.abs(neg))\n",
    "#             counts_pos = np.bincount(pos)\n",
    "        \n",
    "#             if max(counts_neg) > max(counts_pos):\n",
    "#                 tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, -np.argmax(counts_neg), tx_mostFrq[:, index])\n",
    "        \n",
    "#             else:\n",
    "#                 tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, np.argmax(counts_pos), tx_mostFrq[:, index])\n",
    "#         else:\n",
    "#             counts = np.bincount(round_values[0,:])\n",
    "#             tx_mostFrq[:, index] = np.where(tx_mostFrq[:, index] == -999, np.argmax(counts), tx_mostFrq[:, index]) \n",
    "            \n",
    "#     tx_mixed = tX.copy()\n",
    "#     tx_mixed[:, col_index[0]] = tx_mostFrq[:, col_index[0]]\n",
    "#     tx_mixed[:, col_index[1:]] = tx_zeros[:, col_index[1:]]\n",
    "    \n",
    "#     return tx_zeros, tx_mostFrq, tx_mixed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
